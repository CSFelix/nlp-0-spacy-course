{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbb762e0-8d14-41a6-829b-136d1b3da763",
   "metadata": {},
   "source": [
    "<center>    \n",
    "    <h1 id='spacy-chapter-0' style='color:#7159c1; font-size:350%'>Spacy: Chapter 0</h1>\n",
    "    <i style='font-size:125%'>Finding Words, Phrases, Names and Concepts</i>\n",
    "</center>\n",
    "\n",
    "> **Topics**\n",
    "\n",
    "```\n",
    "- ‚öôÔ∏è NLP Processes\n",
    "- üìÅ Spacy Documents, Tokens and Spans\n",
    "- üìù Lexical Attributes\n",
    "- ü™à Pipelines\n",
    "- üè∑Ô∏è Named Entities (NER)\n",
    "- üîç Matches\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88ed602-b62c-4e80-a10b-f8e8cdb42bab",
   "metadata": {},
   "source": [
    "<h1 id='0-nlp-processes' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>‚öôÔ∏è | NLP Processes</h1>\n",
    "\n",
    "`Natural Language Processing (NLP)` is a field of Artificial Intelligence and Machine Learning focused on processing sequential texts. It can be used for a variety of tasks, since in `Phonemes` and `Morphemes & Lexemes` to `Syntax` and `Context` tasks. Applications-wise, we can do `Speech to Text`, `Documents Summary`, `Part-of-Speech (POS) Tagging` projects and much more!!\n",
    "\n",
    "The image below shows some of the applications we can do diving into each block of language.\n",
    "\n",
    "<figure style='text-aling:center'>\n",
    "    <img style='border-radius:20px' src='./images/0-nlp-processes.png' alt='Diagram of possible NLP Applications in each Block of Language' />\n",
    "    <figcaption>Figure 1 - Diagram of possible NLP Applications in each Block of Language. By <a href='https://www.oreilly.com/library/view/practical-natural-language/9781492054047/ch01.html'>Oreilly - Practical Natural Language Book - Chapter 1</a>.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556a1e83-4e0d-4d14-a24f-ce434c4bd257",
   "metadata": {},
   "source": [
    "<h1 id='1-spacy-documents-tokens-and-spans' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üìÅ | Spacy Documents, Tokens and Spans</h1>\n",
    "\n",
    "`Spacy` is a great Python package to work with NLP and in this section we are going to dive into its basic objects: Documents, Tokens and Spans! First off, in order to install the package, we should run the following command:\n",
    "\n",
    "```bash\n",
    "pip install -U spacy\n",
    "```\n",
    "\n",
    "In Spacy, we always create a blank Language Model (LM) or load a pre-trained one. For now, we'll be creating a blank one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1280208-5630-457f-bfe3-f58caa70b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importings\n",
    "import spacy\n",
    "\n",
    "# Creating a blank English NLP Object AKA Processing Pipeline\n",
    "nlp = spacy.blank('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8320c3-b34d-4af6-a3df-1c76b8980882",
   "metadata": {},
   "source": [
    "After creating a Processing Pipeline, we can process sequential texts to create a Document with Tokens. By default, Spacy works with `n-gram` Tokens, that is, each word is considered as a Token.\n",
    "\n",
    "Besides, when two or more tokens are together, they're called Span, so:\n",
    "\n",
    "- **Document** - `object of Tokens`;\n",
    "- **Token** - `a word or punctuation`;\n",
    "- **Span** - `two or more tokens together`.\n",
    "\n",
    "There are some observations about Tokenization to keep in mind:\n",
    "\n",
    "    1. single spaces are not considered as tokens, but multiple spaces are considered as a unique token;\n",
    "    \n",
    "    2. words with hiphen are split into multiple tokens, for instance, the word 'ad-free' is split into three tokens: ['ad', '-', 'free']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "945347bd-324c-4e40-9682-c20ed88f7c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Full Document Text: Hey it is me, Goku!\n",
      "---\n",
      "- Text of Each Token:\n",
      "Hey\n",
      "it\n",
      "is\n",
      "me\n",
      ",\n",
      "Goku\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# Document\n",
    "document = nlp('Hey it is me, Goku!')\n",
    "\n",
    "print(f'- Full Document Text: {document.text}')\n",
    "print('---')\n",
    "\n",
    "print('- Text of Each Token:')\n",
    "for token in document: print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed4b5f3c-3207-4760-8fdf-2fb5da1f7012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Single Token Text: Goku\n"
     ]
    }
   ],
   "source": [
    "# Token\n",
    "token = document[5]\n",
    "print(f'- Single Token Text: {token.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "003545fe-c6fd-4d4f-97c5-1076e09719bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Single Span Token Text: Hey it is me\n"
     ]
    }
   ],
   "source": [
    "# Span\n",
    "span = document[0:4]\n",
    "print(f'- Single Span Token Text: {span.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab652e8-5263-4caf-ada0-7c324647372a",
   "metadata": {},
   "source": [
    "<h1 id='2-lexical-attributes' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üìù | Lexical Attributes</h1>\n",
    "\n",
    "`Lexical Attributes` are info about the words from a language that are `context-independent`, that is, they are not influenced neither by the role of the word in a sentence nor by the context of it.\n",
    "\n",
    "There are lot of Lexical Attributes, so let's just keep in mind the main ones by now:\n",
    "\n",
    "- **i** - `token's index in the document`;\n",
    "- **text** - `token's text in string type`;\n",
    "- **is_alpha** - `whether a token contains only alphabetic characters (a-zA-Z)`;\n",
    "- **is_punct** - `whether a token is a punctuation`;\n",
    "- **is_digit** - `whether a token contains only digits (0-9)`;\n",
    "- **like_num** - `whether a token resemble a number (5 or 'five')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3f3ee74-856d-4831-83c5-2b4c0d35869d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "- Text: ['It', 'costs', '$', '5.00', '.', 'Yeah', ',', 'five', 'dollars', '!']\n",
      "- Is Alphabetic? [True, True, False, False, False, True, False, True, True, False]\n",
      "- Is Punctuation? [False, False, False, False, True, False, True, False, False, True]\n",
      "- Is Digit? [False, False, False, False, False, False, False, False, False, False]\n",
      "- Is Like a Number? [False, False, False, True, False, False, False, True, False, False]\n"
     ]
    }
   ],
   "source": [
    "# Lexical Attributes\n",
    "document = nlp('It costs $5.00. Yeah, five dollars!')\n",
    "\n",
    "print(f'- Index: {[token.i for token in document]}')\n",
    "print(f'- Text: {[token.text for token in document]}')\n",
    "print(f'- Is Alphabetic? {[token.is_alpha for token in document]}')\n",
    "print(f'- Is Punctuation? {[token.is_punct for token in document]}')\n",
    "print(f'- Is Digit? {[token.is_digit for token in document]}')\n",
    "print(f'- Is Like a Number? {[token.like_num for token in document]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d77628f-5534-4c5c-acb4-00054a51d73d",
   "metadata": {},
   "source": [
    "<h1 id='3-pipelines' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>ü™à | Pipelines</h1>\n",
    "\n",
    "Only Lexical Attributes cannot be enough in some projects where we have to get information about `Part-of-Speech (POS)`, `Named Entities (NER)` and `Contexts`, that is, `context-dependent` variables.\n",
    "\n",
    "In these scenarios, we must have a pre-trained model that has learnt the corpus and rules of the target language, and in order to achieve it, we can either create a model from the scratch or load a pre-trained one.\n",
    "\n",
    "Since creating a model from the scratch is not the goal of this notebook and I still don't have the enough knowledge for it (üòÇ), we will be working with English pre-trained models from Spacy.\n",
    "\n",
    "---\n",
    "\n",
    "Actually, Spacy contains four Language Models (LM) in English:\n",
    "\n",
    "- **en_core_web_sm** - `small pipeline and doesn't contain WordVectors`;\n",
    "- **en_core_web_md** - `middle pipeline and contains WordVectors`;\n",
    "- **en_core_web_lg** - `large pipeline and contains WordVectors`;\n",
    "- **en_core_web_trf** - `equivalent to the large pipeline but without WordVectors and more focused on accuracy`.\n",
    "\n",
    "While `sm, md and lg` Pipelines are focused on `Efficiency` and works faster, `trf` Pipeline is focused on `Accuracy` and demands more time and computational cost to process.\n",
    "\n",
    "Since these models are not automatically installed with Spacy, we must to install them manually running the following commands:\n",
    "\n",
    "```bash\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download en_core_web_md\n",
    "python -m spacy download en_core_web_lg\n",
    "python -m spacy download en_core_web_trf\n",
    "```\n",
    "\n",
    "Even though the Pipelines are pre-trained already, we can fine-tune them with our own dataset!\n",
    "\n",
    "---\n",
    "\n",
    "And finally, let's take a look at some of the advantages in using pre-trained Pipelines:\n",
    "\n",
    "- **Part-of-Speech (.pos_) Recognition** - `whether a token is a verb, noun, pronoun...`;\n",
    "- **Dependency Label (.dep_) Recognition** - `what role a token takes in the document accordingly to the context, such as nominal subject, object, root, determinant...`;\n",
    "- **Syntatic Head (.head.text)** - `parent token that is directly related to the current token accordingly to the current token Dependency Label`;\n",
    "- **Stop Word (.is_stop) Detection** - `wheter a token is a Stop Word`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd3fb683-a352-47ba-89b1-802e76c9671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Large Pipeline\n",
    "nlp_large = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eba0f90-87ff-410a-8b34-8b91c6a13568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Text: Hey\n",
      "- Part-of-Speech (POS): INTJ\n",
      "- Dependency Label: intj\n",
      "- Head Token: is\n",
      "- Is Stop Word: False\n",
      "---\n",
      "- Text: it\n",
      "- Part-of-Speech (POS): PRON\n",
      "- Dependency Label: nsubj\n",
      "- Head Token: is\n",
      "- Is Stop Word: True\n",
      "---\n",
      "- Text: is\n",
      "- Part-of-Speech (POS): AUX\n",
      "- Dependency Label: ROOT\n",
      "- Head Token: is\n",
      "- Is Stop Word: True\n",
      "---\n",
      "- Text: me\n",
      "- Part-of-Speech (POS): PRON\n",
      "- Dependency Label: attr\n",
      "- Head Token: is\n",
      "- Is Stop Word: True\n",
      "---\n",
      "- Text: ,\n",
      "- Part-of-Speech (POS): PUNCT\n",
      "- Dependency Label: punct\n",
      "- Head Token: is\n",
      "- Is Stop Word: False\n",
      "---\n",
      "- Text: Goku\n",
      "- Part-of-Speech (POS): PROPN\n",
      "- Dependency Label: attr\n",
      "- Head Token: is\n",
      "- Is Stop Word: False\n",
      "---\n",
      "- Text: !\n",
      "- Part-of-Speech (POS): PUNCT\n",
      "- Dependency Label: punct\n",
      "- Head Token: is\n",
      "- Is Stop Word: False\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Checking Tokens' Context-Dependent Info\n",
    "document = nlp_large('Hey it is me, Goku!')\n",
    "\n",
    "for token in document:\n",
    "    print(f'- Text: {token.text}')\n",
    "    print(f'- Part-of-Speech (POS): {token.pos_}')\n",
    "    print(f'- Dependency Label: {token.dep_}')\n",
    "    print(f'- Head Token: {token.head.text}')\n",
    "    print(f'- Is Stop Word: {token.is_stop}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079ee99f-2a45-4751-992b-78cbbea91e41",
   "metadata": {},
   "source": [
    "<h1 id='4-named-entities-ner' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üè∑Ô∏è | Named Entities (NER)</h1>\n",
    "\n",
    "A common task when dealing with sequential texts is to identify real-world object, such as companies, organizations, people, countries, objects, currencies... This kind of objects are called `Entities` or `Named Entities (NER)` in NLP.\n",
    "\n",
    "All recognized entities from a document can be acessed via the following attributes:\n",
    "\n",
    "- **document.ents** - `returns a list of Spans, where each Span corresponds to an entity`;\n",
    "- **Entity Label (.label_)** - `returns what kind of entity the Span is. We can use 'spacy.explain' method to get a further explanation about the label definition`.\n",
    "\n",
    "Since whether a Token or Span is an entity depends on the context, it's also needed to work with a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2caf07e-a969-4aa0-a1f5-626e4802ec33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Entity Text: Apple\n",
      "- Label: ORG\n",
      "- Explanation: Companies, agencies, institutions, etc.\n",
      "---\n",
      "- Entity Text: U.K.\n",
      "- Label: GPE\n",
      "- Explanation: Countries, cities, states\n",
      "---\n",
      "- Entity Text: $1 billion\n",
      "- Label: MONEY\n",
      "- Explanation: Monetary values, including unit\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Listing Entities\n",
    "document = nlp_large('Apple is looking at buying U.K. startup at $1 billion.')\n",
    "\n",
    "for entity in document.ents:\n",
    "    print(f'- Entity Text: {entity.text}')\n",
    "    print(f'- Label: {entity.label_}')\n",
    "    print(f'- Explanation: {spacy.explain(entity.label_)}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ced3173-5fe9-43a6-b88a-f3828e593c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- GPE: Countries, cities, states\n",
      "- NNP: noun, proper singular\n",
      "- dobj: direct object\n"
     ]
    }
   ],
   "source": [
    "# We can also call 'spacy.explain' method to get further info about Spacy's abbreviations\n",
    "print(f'- GPE: {spacy.explain(\"GPE\")}')\n",
    "print(f'- NNP: {spacy.explain(\"NNP\")}')\n",
    "print(f'- dobj: {spacy.explain(\"dobj\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486eb061-b678-4676-89c7-dfe5cc0a45d4",
   "metadata": {},
   "source": [
    "However, there are some situations where entities are not identified automatically by Spacy. In these situations, we must get them as Spans manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbd8ad0d-eef6-4197-a0ed-606eda9d4627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Entity Text: Apple\n",
      "- Label: ORG\n",
      "- Explanation: Companies, agencies, institutions, etc.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Getting Entities Manually (1)\n",
    "text = 'Upcoming iPhone X release date leaked as Apple reveals pre-orders'\n",
    "document = nlp_large(text)\n",
    "\n",
    "for entity in document.ents:\n",
    "    print(f'- Entity Text: {entity.text}')\n",
    "    print(f'- Label: {entity.label_}')\n",
    "    print(f'- Explanation: {spacy.explain(entity.label_)}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ca1e4fa-6d84-4e1a-926b-f0c42310c44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Missing Entity: iPhone X\n"
     ]
    }
   ],
   "source": [
    "# Getting Entities Manually (2)\n",
    "iphone_x_entity = document[1:3]\n",
    "print(f'- Missing Entity: {iphone_x_entity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d303d4-dba9-4f1b-a53a-750c63f5b794",
   "metadata": {},
   "source": [
    "We will learn better ways to fetch non-recognized entities and even make the model learn to automatically identify them in the further lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53b132b-e188-40ca-a123-a2a566a1719c",
   "metadata": {},
   "source": [
    "<h1 id='5-matches' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üîç | Matches</h1>\n",
    "\n",
    "Sometimes we desire to apply rules in order to find words or phrases into text and are in these situations that `Matches` and `RegEx` come in action.\n",
    "\n",
    "While `RegEx` searches contents only into strings, `Matches` allow us to search content into Spacy Documents, Tokens and Spans, applying advanced linguistic searches, such as, match the word 'duck' only when it's a noun and not a verb.\n",
    "\n",
    "Matches are passed as a list of dictionaries, where each dictionary corresponds to a single token search rule. Besides, search rules are applied sequentially in the Tokens.\n",
    "\n",
    "Oh, and there are some filter Operators that we can use with Matcher:\n",
    "\n",
    "- **{ 'OP': '!' }** - `Negation - Match 0 times`;\n",
    "- **{ 'OP': '?' }** - `Optional - Match 0 or 1 times`;\n",
    "- **{ 'OP': '+' }** - `Match 1 or more times`;\n",
    "- **{ 'OP': '*' }** - `Match 0 or more times`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95cac724-0308-47ef-bf5b-e34b4c6a7b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matches (1)\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "pattern1 = [{ 'TEXT': 'iPhone' }, { 'TEXT': 'X' }] # matches a token with text 'iPhone' followed by a token with text 'X'\n",
    "pattern2 = [{ 'LOWER': 'iphone' }, { 'LOWER': 'x' }] # matches a token with lowercase text 'iphone' followed by a token with lowercase text 'x'\n",
    "pattern3 = [{ 'LEMMA': 'buy' }, { 'POS': 'NOUN' }] # matches a token with lemma (dictionary form) as 'buy' followed by a token with part-of-speech (pos) as 'NOUN'. For instance, 'buy flowers' and 'bought books'\n",
    "\n",
    "matcher = Matcher(nlp_large.vocab) # always feed Matcher with Vocab when instantiating it\n",
    "matcher.add('IPHONE_PATTERN_1', [pattern1])\n",
    "matcher.add('IPHONE_PATTERN_2', [pattern2])\n",
    "matcher.add('IPHONE_PATTERN_3', [pattern3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3040e80-11dd-4dd3-b5c3-3ab836f105b7",
   "metadata": {},
   "source": [
    "Matcher returns a list of tuples containing the following three elements each:\n",
    "\n",
    "- **match_id** - `match id (a random hash)`;\n",
    "- **start** - `start index of the matched Span into the Document`;\n",
    "- **end** - `end index of the matched Span into the Document`.\n",
    "\n",
    "Also, we can create a Span of the match by slicing the Document by 'start' and 'end' indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "717bd1ca-8aa4-47e2-b9b6-19aecda20192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Match ID: 7162708591567205619\n",
      "- Matched Span: iPhone X\n",
      "---\n",
      "- Match ID: 16007330697362006008\n",
      "- Matched Span: iPhone X\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "matches = matcher(document)\n",
    "\n",
    "for match_id, start_index, end_index in matches:\n",
    "    matched_span = document[start_index: end_index]\n",
    "\n",
    "    print(f'- Match ID: {match_id}')\n",
    "    print(f'- Matched Span: {matched_span.text}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58911df2-91d6-4685-9da7-6d8883b758bb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's do some exercises with Matches now!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "571df88b-cf19-4ed8-8df3-d5be0a2bde2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Tournament' Name: 2018 FIFA World Cup:\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1) Extract the name of the tournament\n",
    "document = nlp_large('2018 FIFA World Cup: France won!')\n",
    "\n",
    "pattern_fifa = [\n",
    "    { 'IS_DIGIT': True }\n",
    "    , { 'LOWER': 'fifa' }\n",
    "    , { 'LOWER': 'world' }\n",
    "    , { 'LOWER': 'cup' }\n",
    "    , { 'IS_PUNCT': True }\n",
    "]\n",
    "\n",
    "matcher = Matcher(nlp_large.vocab)\n",
    "matcher.add('FIFA_PATTERN', [pattern_fifa])\n",
    "matches = matcher(document)\n",
    "\n",
    "for match_id, start_index, end_index in matches:\n",
    "    matched_span = document[start_index:end_index]\n",
    "    print(f'- Tournament\\' Name: {matched_span.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e6317f9-cc86-4fa9-81eb-c6d68dd74be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Matched Span: loved cats\n",
      "- Matched Span: love dogs\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2) Apply two filters in a single token\n",
    "document = nlp_large('I loved cats but I love dogs more')\n",
    "\n",
    "pattern_animals = [{ 'LEMMA': 'love', 'POS': 'VERB' }, { 'POS': 'NOUN' }]\n",
    "\n",
    "matcher = Matcher(nlp_large.vocab)\n",
    "matcher.add('ANIMALS_SEARCH', [pattern_animals])\n",
    "matches = matcher(document)\n",
    "\n",
    "for match_id, start_index, end_index in matches:\n",
    "    matched_span = document[start_index:end_index]\n",
    "    print(f'- Matched Span: {matched_span.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "589284d5-722b-49d2-b2b1-ca8358fd8525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Matched Span: bought a smartphone\n",
      "- Matched Span: buying apps\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3) Apply filters using operators\n",
    "document = nlp_large('I bought a smartphone. Now I am buying apps.')\n",
    "\n",
    "pattern_with_operators = [\n",
    "    { 'LEMMA': 'buy' }\n",
    "    , { 'POS': 'DET', 'OP': '?' }\n",
    "    , { 'POS': 'NOUN' }\n",
    "]\n",
    "\n",
    "matcher = Matcher(nlp_large.vocab)\n",
    "matcher.add('PATTERN_WITH_OPERATORS', [pattern_with_operators])\n",
    "matches = matcher(document)\n",
    "\n",
    "for match_id, start_index, end_index in matches:\n",
    "    matched_span = document[start_index:end_index]\n",
    "    print(f'- Matched Span: {matched_span.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eece0e04-701d-4906-a37f-f46c744a3d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Matched Span: love cats\n",
      "- Matched Span: very happy\n",
      "- Matched Span: very very happy\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4) Applu filters using operators again\n",
    "document = nlp_large('I love cats and I am very very happy')\n",
    "\n",
    "pattern_love_cats = [{ 'LEMMA': 'love', 'POS': 'VERB' }, { 'LOWER': 'cats' }]\n",
    "pattern_very_happy = [{ 'LOWER': 'very', 'OP': '+' }, { 'LOWER': 'happy' }]\n",
    "\n",
    "matcher = Matcher(nlp_large.vocab)\n",
    "matcher.add('PATTERN_LOVE_CATS', [pattern_love_cats])\n",
    "matcher.add('PATTERN_VERY_HAPPY', [pattern_very_happy])\n",
    "matches = matcher(document)\n",
    "\n",
    "for match_id, start_index, end_index in matches:\n",
    "    matched_span = document[start_index:end_index]\n",
    "    print(f'- Matched Span: {matched_span.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3941a09-f1d3-4664-a873-527791aadd9b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h1 id='reach-me' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üì´ | Reach Me</h1>\n",
    "\n",
    "> **Email** - [csfelix08@gmail.com](mailto:csfelix08@gmail.com?)\n",
    "\n",
    "> **Linkedin** - [linkedin.com/in/csfelix/](https://www.linkedin.com/in/csfelix/)\n",
    "\n",
    "> **GitHub:** - [CSFelix](https://github.com/CSFelix)\n",
    "\n",
    "> **Kaggle** - [DSFelix](https://www.kaggle.com/dsfelix)\n",
    "\n",
    "> **Portfolio** - [CSFelix.io](https://csfelix.github.io/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
