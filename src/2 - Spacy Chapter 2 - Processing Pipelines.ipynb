{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05efd590-b0a8-4c9a-9c0c-e429e606acc9",
   "metadata": {},
   "source": [
    "<center>    \n",
    "    <h1 id='spacy-chapter-2' style='color:#7159c1; font-size:350%'>Spacy: Chapter 2</h1>\n",
    "    <i style='font-size:125%'>Processing Pipelines</i>\n",
    "</center>\n",
    "\n",
    "> **Topics**\n",
    "\n",
    "```\n",
    "- ðŸªˆ Pipelines - Part II\n",
    "- ðŸŽ¨ Custom Pipelines Components\n",
    "- ðŸ§© Extension Types: Attributes, Properties and Methods\n",
    "- ðŸ“ˆ Scaling and Performance\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4288298-e644-4300-82e4-968ecb41add3",
   "metadata": {},
   "source": [
    "<h1 id='0-pipelines-part-ii' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>ðŸªˆ | Pipelines Part (II)</h1>\n",
    "\n",
    "When a text is converted into a Document in Spacy, it's automatically processed by `Pipelines` in order to extract information about it, such as lemma, Part-of-Speech, Dependency Label and Entities.\n",
    "\n",
    "Normally, pre-trained models in Spacy contain the following seven Pipelines:\n",
    "\n",
    "- **tokenizer** - `transforms each word in a Token`;\n",
    "- **tok2vec** - `calculates the WordVector for the whole Document and for each Token. Word2Vec is the default algorithm used for this task in Spacy`;\n",
    "- **tagger** - `responsible to assign Tag and Part-of-Speech (POS) on each Token, that is, the grammatical role`;\n",
    "- **parser** - `responsible to assign the relationships of the Tokens in the text, such as Dependency Label and Syntatic Head`;\n",
    "- **lemmatizer** - `responsible to assign the Lemma (dictionary/base form) to Tokens`;\n",
    "- **attribute_ruler** - `responsible to process Tokens and assign information on them following specific rules and logic given by us. This Pipeline is normally used when Spacy cannot process well a certain word, phrase or a target language`;\n",
    "- **ner (Named Entities)** - `responsible to identify and assign Named Entities and Labels`;\n",
    "- **textcat** - `responsible to assign categories to Documents following rules and logic given by us. This Pipeline is normally used on Text Classification projects. For instance, the rating 'Steins Gate; is an amazing show' should be classified as 'positive'`.\n",
    "\n",
    "The two images below illustrates the Pipelines architecture in Spacy:\n",
    "\n",
    "<figure style='text-aling:center'>\n",
    "    <img style='border-radius:20px' src='./images/2.0-text-processing.png' alt='Diagram of Pipelines Architecture in Spacy' />\n",
    "    <figcaption>Figure 1 - Diagram of Pipelines Architecture in Spacy. By <a href='https://course.spacy.io/en/chapter3'>Spacy - Advanced NLP with Spacy Course - Chapter 3</a>.</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure style='text-aling:center'>\n",
    "    <img style='border-radius:20px' src='./images/2.1-built-in-pipelines-of-text-processing.png' alt='Table of Pipelines Roles in Spacy' />\n",
    "    <figcaption>Figure 2 - Table of Pipelines's Roles in Spacy. By <a href='https://course.spacy.io/en/chapter3'>Spacy - Advanced NLP with Spacy Course - Chapter 3</a>.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b4e6ca4-5a10-4d17-8fb0-22c92b814eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing Pipelines Architecture\n",
    "import spacy\n",
    "\n",
    "nlp_sm = spacy.load('en_core_web_sm')\n",
    "nlp_md = spacy.load('en_core_web_md')\n",
    "nlp_lg = spacy.load('en_core_web_lg')\n",
    "nlp_trf = spacy.load('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beb48e20-7967-405b-8636-d7d8aac07ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Small Model Pipelines: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "- Medium Model Pipelines: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "- Large Model Pipelines: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "- TRF Model Pipelines: ['transformer', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# Listing Pipelines Architecture (List of Pipeline' Names)\n",
    "print(f'- Small Model Pipelines: {nlp_sm.pipe_names}')\n",
    "print(f'- Medium Model Pipelines: {nlp_md.pipe_names}')\n",
    "print(f'- Large Model Pipelines: {nlp_lg.pipe_names}')\n",
    "print(f'- TRF Model Pipelines: {nlp_trf.pipe_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48eb93d1-9c1f-446c-a802-ff1e0d4e829b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Small Model Pipelines: [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x000001B8FB62BC50>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x000001B8FB62AC90>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x000001B8FB4DEE30>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x000001B8FB879D10>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x000001B8FB878090>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x000001B8FB4DEF80>)]\n",
      "- Medium Model Pipelines: [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x000001B8BF491DF0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x000001B8C06B04D0>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x000001B8D4961850>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x000001B8BF996750>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x000001B8BF996FD0>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x000001B8D4961E00>)]\n",
      "- Large Model Pipelines: [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x000001B8D6E67D70>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x000001B8BC2C7BF0>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x000001B8C72EEF10>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x000001B8C0B008D0>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x000001B8C0773350>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x000001B8C72EFA70>)]\n",
      "- TRF Model Pipelines: [('transformer', <spacy_curated_transformers.pipeline.transformer.CuratedTransformer object at 0x000001B8C06B3B30>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x000001B8C06B2BD0>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x000001B8D4960EB0>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x000001B8D0F5D1D0>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x000001B8D4906990>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x000001B8D4960C10>)]\n"
     ]
    }
   ],
   "source": [
    "# Listing Pipelines Architecture (Tuple of Pipeline' Names and Pipeline's Objects)\n",
    "print(f'- Small Model Pipelines: {nlp_sm.pipeline}')\n",
    "print(f'- Medium Model Pipelines: {nlp_md.pipeline}')\n",
    "print(f'- Large Model Pipelines: {nlp_lg.pipeline}')\n",
    "print(f'- TRF Model Pipelines: {nlp_trf.pipeline}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb05580-12d9-42dc-bcb5-f5b2eb180290",
   "metadata": {},
   "source": [
    "<h1 id='1-custom-pipeline-components' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>ðŸŽ¨ | Custom Pipeline Components</h1>\n",
    "\n",
    "Spacy allows us to create and add our own functions into its Pipeline Architecture in order to add specific rules and logic to process texts. With this, we are able to do a variety of things, such as, modify the Document, add more data to Tokens and even update the Named Entities (NER) list.\n",
    "\n",
    "These added functions are considered as `Custom Pipeline Components`, also known as `Components`, and they are Pipelines that must modify and return a Document. Besides, in order to be considered as a Component, we must add `@Language.component` decorator before defining the function.\n",
    "\n",
    "After creating our Component, we are able to add it into Model's Pipeline Architecture. When adding it, we can specify one of the four available positions:\n",
    "\n",
    "- **last (default behaviour)** - `if True, the Component will be added in the end of the architecture`;\n",
    "- **first** - `if True, the Component will be added in the beginning of the architecture, right after the Tokenizer Pipeline`;\n",
    "- **before** - `the Component will be added right before the specified Pipeline`;\n",
    "- **after** - `the Component will be added right after the specified Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "264b459a-772b-4217-bee4-4eca6ecbddf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Pipelines: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'custom_component']\n",
      "- Document Length: 7\n"
     ]
    }
   ],
   "source": [
    "# Custom Pipeline Components\n",
    "from spacy.language import Language\n",
    "\n",
    "@Language.component('custom_component')\n",
    "def length_component(document):\n",
    "    print(f'- Document Length: {len(document)}')\n",
    "    return document\n",
    "\n",
    "nlp_lg.add_pipe('custom_component') # default behavior: last=True\n",
    "print(f'- Pipelines: {nlp_lg.pipe_names}')\n",
    "\n",
    "document = nlp_lg('Hey it is me, Goku!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4967d534-100f-46e8-b892-cafbed88c737",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "964ccc3b-47b2-4384-bd13-e40e383ecdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Entity Text: Golden Retriever\n",
      "- Entity Label: ANIMAL\n",
      "- Label Explanation: None\n",
      "---\n",
      "- Entity Text: cat\n",
      "- Entity Label: ANIMAL\n",
      "- Label Explanation: None\n",
      "---\n",
      "- Entity Text: turtle\n",
      "- Entity Label: ANIMAL\n",
      "- Label Explanation: None\n",
      "---\n",
      "- Entity Text: bunny\n",
      "- Entity Label: ANIMAL\n",
      "- Label Explanation: None\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\spacy\\glossary.py:20: UserWarning: [W118] Term 'ANIMAL' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
      "  warnings.warn(Warnings.W118.format(term=term))\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1) Create a Custom Component to search for Animals and then\n",
    "# update the Document Entities in order to contain only the matched\n",
    "# Animals\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp_large = spacy.load('en_core_web_lg')\n",
    "\n",
    "animals = ['Golden Retriever', 'cat', 'turtle', 'bunny']\n",
    "animals_pattern = nlp_large.pipe(animals)\n",
    "\n",
    "matcher = PhraseMatcher(nlp_large.vocab)\n",
    "matcher.add('ANIMALS', animals_pattern)\n",
    "\n",
    "@Language.component('animals_component')\n",
    "def animals_component(document):\n",
    "    matches = matcher(document)\n",
    "    spans = [\n",
    "        Span(document, start_index, end_index, label='ANIMAL')\n",
    "        for match_id, start_index, end_index in matches\n",
    "    ]\n",
    "    document.ents = spans\n",
    "    return document\n",
    "\n",
    "nlp_large.add_pipe('animals_component', after='ner')\n",
    "\n",
    "document = nlp_large('I have a Golden Retriever, also a cat, a turtle and a little bunny.')\n",
    "\n",
    "for entity in document.ents:\n",
    "    print(f'- Entity Text: {entity.text}')\n",
    "    print(f'- Entity Label: {entity.label_}')\n",
    "    print(f'- Label Explanation: {spacy.explain(entity.label_)}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8be164-50b2-4b74-814e-c3a1180e1736",
   "metadata": {},
   "source": [
    "<h1 id='2-extension-types-attributes-properties-and-methods' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>ðŸ§© | Extension Types: Attributes, Properties and Methods</h1>\n",
    "\n",
    "Now let's see how we can add custom `Attributes, Properties and Methods` into Documents, Tokens and Spans. All custom extensions added into them are accessible with '_.', telling that the Attribute/Property/Method being accessed is a custom one created by us instead of by Spacy.\n",
    "\n",
    "The three types of Custom Extensions are:\n",
    "\n",
    "- **Attributes** - `variables with a default value set to it and allow us to overwrite their values`;\n",
    "- **Properties** - `functions that automatically sets a value to Attributes accordingly to our custom logic. We can define a required 'getter' and an optional 'setter'`;\n",
    "- **Methods** - `functions that returns a value accordingly to our custom logic. They don't create an accessible Attribute, but yes, an accessible Function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98bc366d-438a-446a-899d-4a07d3e7d25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc, Token, Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab8d2d2f-0a7c-45b6-a039-24080b9b97d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribute Extensions\n",
    "Doc.set_extension('title', default=None)\n",
    "Token.set_extension('is_color', default=False)\n",
    "Span.set_extension('has_color', default=False)\n",
    "\n",
    "document = nlp_large('My favorite colors are purple, black and gold!')\n",
    "token = document[4]\n",
    "span = document[0:3]\n",
    "\n",
    "document._.title = 'Favorite Colors'\n",
    "token._.is_color = True\n",
    "span._.has_color = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2050050-51df-4ca9-8c76-d41f4e08eb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Is Token a favorite color? True - purple\n"
     ]
    }
   ],
   "source": [
    "# Property Extensions - Tokens\n",
    "def get_is_favorite_color(token):\n",
    "    colors = ['purple', 'black', 'gold']\n",
    "    return token.text in colors\n",
    "\n",
    "Token.set_extension('is_favorite_color', getter=get_is_favorite_color)\n",
    "token = document[4]\n",
    "print(f'- Is Token a favorite color? {token._.is_favorite_color} - {token.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d62d6053-b99a-41cb-b730-77648fc65ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does Span 1 contain a favorite color? True - My favorite colors are purple,\n",
      "Does Span 2 contain a favorite color? True - and gold!\n"
     ]
    }
   ],
   "source": [
    "# Propety Extensions - Spans\n",
    "def get_has_favorite_color(span):\n",
    "    colors = ['purple', 'black', 'gold']\n",
    "    return any(token.text in colors for token in span)\n",
    "\n",
    "Span.set_extension('has_favorite_color', getter=get_has_favorite_color)\n",
    "span1 = document[0:6]\n",
    "span2 = document[7:10]\n",
    "print(f'Does Span 1 contain a favorite color? {span1._.has_favorite_color} - {span1.text}')\n",
    "print(f'Does Span 2 contain a favorite color? {span2._.has_favorite_color} - {span2.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45a51480-2778-4f7e-8c99-3946fec1ce0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does Document contain the purple color? True\n",
      "Does Document contain the red color? False\n"
     ]
    }
   ],
   "source": [
    "# Method Extensions\n",
    "def has_token(document, token_text):\n",
    "    return token_text in [token.text for token in document]\n",
    "\n",
    "Doc.set_extension('has_token', method=has_token)\n",
    "print(f'Does Document contain the purple color? {document._.has_token(\"purple\")}')\n",
    "print(f'Does Document contain the red color? {document._.has_token(\"red\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1390fbdd-28fc-4902-a1b8-60a942b1b220",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3f553ea-7073-4cfc-b710-45315aede9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- fifty years: None\n",
      "- first: None\n",
      "- David Bowie: https://en.wikipedia.org/w/index.php?search=David_Bowie\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2) Generate Wikipedia URLs for Person, Organization,\n",
    "# Country and Location Spans\n",
    "nlp_large = spacy.load('en_core_web_lg')\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    if span.label_ in ['PERSON', 'ORG', 'GPE', 'LOCATION']:\n",
    "        entity_text = span.text.replace(' ', '_')\n",
    "        return 'https://en.wikipedia.org/w/index.php?search=' + entity_text\n",
    "\n",
    "Span.set_extension('wikipedia_url', getter=get_wikipedia_url)\n",
    "\n",
    "document = nlp_large(\n",
    "    'In over fifty years from his very first recordings right through to his '\n",
    "    'last album, David Bowie was at the vanguard of contemporary culture.'\n",
    ")\n",
    "\n",
    "for entity in document.ents: print(f'- {entity.text}: {entity._.wikipedia_url}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fd0f73-560d-48f7-9809-01c113ef64f6",
   "metadata": {},
   "source": [
    "<h1 id='3-scaling-and-performance' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>ðŸ“ˆ | Scaling and Performance</h1>\n",
    "\n",
    "Now, let's see some tips to boost our NLP tasks!!\n",
    "\n",
    "```\n",
    "- nlp.pipe\n",
    "- Passing in Context\n",
    "- Using only the Tokenizer\n",
    "- Disabling Pipeline Components\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4696ea2-d914-40ba-8f25-73d2610deb07",
   "metadata": {},
   "source": [
    "- **nlp.pipe** - `useful when processing multiple phrases and yielding multiple Documents`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d3ef17e-de4a-41af-a8a1-3950342eecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp.pipe\n",
    "texts = ['Hey it is me, Goku!', 'You look strong, let\\'s fight!']\n",
    "\n",
    "# Bad Way\n",
    "documents1 = [nlp_large(text) for text in texts]\n",
    "\n",
    "# Good Way\n",
    "documents2 = list(nlp_large.pipe(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e05e2b-9165-4fab-a64d-791cfa321c2a",
   "metadata": {},
   "source": [
    "- **Passing in Context (Part I)** - `when passing 'as_tuples' parameter as 'True' into 'nlp.pipe', we can add additional metadata to the Document`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e805411f-7e7f-4437-a9de-83d9d4139caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Document: Hey it is me, Goku!\n",
      "- Context ID: 1\n",
      "- Context Page Number: 7\n",
      "---\n",
      "- Document: You look strong, let's fight!\n",
      "- Context ID: 2\n",
      "- Context Page Number: 8\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Passing in Context (1)\n",
    "texts = [\n",
    "    ('Hey it is me, Goku!', { 'id': 1, 'page_number': 7 })\n",
    "    , ('You look strong, let\\'s fight!', { 'id': 2, 'page_number': 8 })\n",
    "]\n",
    "\n",
    "for document, context in nlp_large.pipe(texts, as_tuples=True):\n",
    "    print(f'- Document: {document.text}')\n",
    "    print(f'- Context ID: {context[\"id\"]}')\n",
    "    print(f'- Context Page Number: {context[\"page_number\"]}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d3901-cd0a-485e-abd5-bf590cd073cd",
   "metadata": {},
   "source": [
    "- **Passing in Context (Part II)** - `when working with Attribute and Property Extensions, Context may be handy to add values to the Extensions`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1a0ed2f-fb24-47d0-a040-6854a8117aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Document's ID: 1\n",
      "- Document's Page Number: 7\n",
      "---\n",
      "- Document's ID: 2\n",
      "- Document's Page Number: 8\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Passing in Context (2)\n",
    "Doc.set_extension('id', default=None)\n",
    "Doc.set_extension('page_number', default=None)\n",
    "\n",
    "texts = [\n",
    "    ('Hey it is me, Goku!', { 'id': 1, 'page_number': 7 })\n",
    "    , ('You look strong, let\\'s fight!', { 'id': 2, 'page_number': 8 })\n",
    "]\n",
    "\n",
    "for document, context in nlp_large.pipe(texts, as_tuples=True):\n",
    "    document._.id = context['id']\n",
    "    document._.page_number = context['page_number']\n",
    "    print(f'- Document\\'s ID: {document._.id}')\n",
    "    print(f'- Document\\'s Page Number: {document._.page_number}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fd9a64-77f8-4b93-8d35-cad5deaf5877",
   "metadata": {},
   "source": [
    "- **Using only the Tokenizer** - `sometimes we desire to just use the Tokenizer Pipeline on Documents, skipping all the other ones, such as Tok2Vec, Tagger, Parser, NER and Lemmatizer`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f756c86-938d-4f2c-8ee9-c0586685e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing Text with All Pipelines\n",
    "document1 = nlp_large('Hey it is me, Goku!')\n",
    "\n",
    "# Processing Text with Tokenizer Only\n",
    "document2 = nlp_large.make_doc('Hey it is me, Goku!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f1ed85-6195-4644-a637-61fd3881c7bd",
   "metadata": {},
   "source": [
    "- **Disabling Pipeline Components** - `other times we desire to process text with Tokenizer and some specific Pipelines, such as Tagger only`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70343655-7a2f-44a3-a105-b3b3ed6cb7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Tokenizer and Tagger and Lemmatizer Only\n",
    "with nlp_large.select_pipes(enable=['tagger', 'lemmatizer']):\n",
    "    document1 = nlp_large('Hey it is me, Goku!')\n",
    "\n",
    "# Running All Pipelines, but Tagger and Lemmatizer\n",
    "with nlp_large.select_pipes(disable=['tagger', 'lemmatizer']):\n",
    "    document2 = nlp_large('Hey it is me, Goku!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a22676c-4672-4326-a648-9d5a6f152395",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h1 id='reach-me' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>ðŸ“« | Reach Me</h1>\n",
    "\n",
    "> **Email** - [csfelix08@gmail.com](mailto:csfelix08@gmail.com?)\n",
    "\n",
    "> **Linkedin** - [linkedin.com/in/csfelix/](https://www.linkedin.com/in/csfelix/)\n",
    "\n",
    "> **GitHub:** - [CSFelix](https://github.com/CSFelix)\n",
    "\n",
    "> **Kaggle** - [DSFelix](https://www.kaggle.com/dsfelix)\n",
    "\n",
    "> **Portfolio** - [CSFelix.io](https://csfelix.github.io/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
