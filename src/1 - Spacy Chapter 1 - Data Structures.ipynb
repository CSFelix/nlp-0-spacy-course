{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c89d5dd8-ec09-4472-a384-7b6ff639e799",
   "metadata": {},
   "source": [
    "<center>    \n",
    "    <h1 id='spacy-chapter-1' style='color:#7159c1; font-size:350%'>Spacy: Chapter 1</h1>\n",
    "    <i style='font-size:125%'>Data Structures</i>\n",
    "</center>\n",
    "\n",
    "> **Topics**\n",
    "\n",
    "```\n",
    "- üìñ Vocab, StringStore and Lexeme\n",
    "- üìÅ Documents, Tokens and Spans (Part II)\n",
    "- ü™û Word Vectors and Semantic Similarity\n",
    "- üé® Combining Predictions and Rules\n",
    "- üîç PhraseMatcher\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775cb3e5-0ed9-46c9-b59d-44e4192eb015",
   "metadata": {},
   "source": [
    "<h1 id='0-vocab-stringstore-and-lexeme' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üìñ | Vocab, StringStore and Lexeme</h1>\n",
    "\n",
    "`Vocab`, `StringStore` and `Lexeme`: what are they? What is their relationship? Why are they so important to Spacy's architecture?\n",
    "\n",
    "Before creating Documents with Spacy, our pipeline must know a considerable amount of words from our target language in order to be able to identify the Part-of-Speech (POS), Dependency Label, Syntatic Head and so on of each Token.\n",
    "\n",
    "All words are stored into `StringStore` object and, in order to save memory and avoid duplicated entries, each word is stored only once and then is assigned to a string hash.\n",
    "\n",
    "Consequently, each string hash is stored into `Vocab` object, where each element is called `Lexeme`. Since Vocab only stores words hashes independetly, that is, without context and sequential texts, Lexemes contain only `context-independent` info, such as whether a token is a digit, alphabetic or punctuation character.\n",
    "\n",
    "There's alse the `Document` object, where each element is a `Token` and since Document stores sequential texts with context, each Token contains `context-dependent` info, such as Part-of-Speech (POS), Dependency Label, Flag to Stop Words and Syntatic Head.\n",
    "\n",
    "In a nutshell, `StringStore` stores words as string only once and each word is assigned to a string hash. This very hash is stored into `Vocab` as a `Lexeme` object containing all `context-independent` info of it.\n",
    "\n",
    "Besides, every time we create a `Document` in Spacy, the Document accesses Vocab in order to check the existence of the word. If it exists, Spacy gets the hash and search for the word into StringStore in order to associate it to the Document. If it doesn't exist, the word is inserted right away into Vocab and StringStore, then Spacy associate the word to the Document.\n",
    "\n",
    "So, we can say that each time we search for a word, Spacy looks for its hash value into Vocab and then for its string representation into StringStore.\n",
    "\n",
    "The image below illustrates the association between these objects:\n",
    "\n",
    "<figure style='text-aling:center'>\n",
    "    <img style='border-radius:20px' src='./images/1-spacy-architecture.png' alt='Spacy Architecture to Store Words' />\n",
    "    <figcaption>Figure 1 - Spacy Architecture to Store Words By <a href='https://course.spacy.io/en/chapter2'>Spacy - Advanced NLP with Spacy Course - Chapter 2</a>.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6327dd4c-f3c4-4915-81d6-e382dd0a4712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blank models are instatiated with empty Vocab and StringStore,\n",
    "# whereas pre-trained model are instantiated with both objects\n",
    "# populated.\n",
    "#\n",
    "# In both models, blank and pre-trained, new words are automatically\n",
    "# inserted into Vocab and StringStore while creating Documents.\n",
    "#\n",
    "import spacy\n",
    "nlp_blank = spacy.blank('en')\n",
    "document = nlp_blank('I love Natural Language Processing!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c82d0938-1c4b-4531-8816-1c90c4a091cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Love Hash: 3702023516439754181\n",
      "- Love String: love\n"
     ]
    }
   ],
   "source": [
    "# Getting Hash and String of a word\n",
    "love_hash = nlp_blank.vocab.strings['love']\n",
    "love_string = nlp_blank.vocab.strings[love_hash]\n",
    "\n",
    "print(f'- Love Hash: {love_hash}')\n",
    "print(f'- Love String: {love_string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a495160-9232-4d55-b8db-5ca2300bbd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Hate Hash: 8706232279129489120\n",
      "- Hate String: hate\n"
     ]
    }
   ],
   "source": [
    "# Adding a new Word into Vocab and StringStore\n",
    "nlp_blank.vocab.strings.add('hate')\n",
    "\n",
    "hate_hash = nlp_blank.vocab.strings['hate']\n",
    "hate_string = nlp_blank.vocab.strings[hate_hash]\n",
    "\n",
    "print(f'- Hate Hash: {hate_hash}')\n",
    "print(f'- Hate String: {hate_string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f51c51c-0d8a-4992-bd21-ee86a3ca8f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Document Love Hash: 3702023516439754181\n",
      "- Document Love String: love\n"
     ]
    }
   ],
   "source": [
    "# Accessing Directly via Document\n",
    "document_love_hash = document.vocab.strings['love'] # Documents also contains its own 'vocab' and 'StringStore' objects\n",
    "document_love_string = document.vocab.strings[document_love_hash]\n",
    "\n",
    "print(f'- Document Love Hash: {document_love_hash}')\n",
    "print(f'- Document Love String: {document_love_string}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115e6a9c-eb4c-4cf2-858a-320154787a1b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56ae68df-db14-478f-a140-8bea969a7615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Text: love\n",
      "- Hash Value: 3702023516439754181\n",
      "- Some Lexical Attributes:\n",
      "\t- Is Alphabetic? True\n",
      "\t- Is Punctuation? False\n",
      "\t- Is Digit? False\n",
      "\t- Is Like a Number? False\n"
     ]
    }
   ],
   "source": [
    "# Exploring Lexemes\n",
    "document = nlp_blank('I love Natural Language Processing!')\n",
    "lexeme = nlp_blank.vocab['love']\n",
    "\n",
    "print(f'- Text: {lexeme.text}')\n",
    "print(f'- Hash Value: {lexeme.orth}')\n",
    "print(f'- Some Lexical Attributes:')\n",
    "print(f'\\t- Is Alphabetic? {lexeme.is_alpha}')\n",
    "print(f'\\t- Is Punctuation? {lexeme.is_punct}')\n",
    "print(f'\\t- Is Digit? {lexeme.is_digit}')\n",
    "print(f'\\t- Is Like a Number? {lexeme.like_num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ec0d01-9e2f-4a21-a4e9-daf849f6f4b0",
   "metadata": {},
   "source": [
    "<h1 id='1-documents-tokens-and-spans-part-ii' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üìÅ | Documents, Tokens and Spans (Part II)</h1>\n",
    "\n",
    "Recapping what we already know about these three objects:\n",
    "\n",
    "- **Documents** - `object of Tokens, that is, a sequential text with a context`;\n",
    "- **Token** - `a word or a punctuation and each element into a Document`;\n",
    "- **Span** - `slices of the Document, consisting in two or more Tokens together`.\n",
    "\n",
    "We have seen that Documents are created automatically when processing texts with `nlp` object like below:\n",
    "\n",
    "```python\n",
    "document = nlp('Hey it is me, Goku!')\n",
    "```\n",
    "\n",
    "However, we can create them manually passing only three parameters:\n",
    "\n",
    "- **Vocab** - `Vocab object of the target language`;\n",
    "- **Words** - `list of the sequential text where each element is a Token`;\n",
    "- **Spaces** - `list of flags telling whether there is a space right after the corresponding word of the same index into 'words' parameter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01f6d8c0-bf16-463f-83d0-fad2f4a86767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Document Text: Hey it is me, Goku!\n"
     ]
    }
   ],
   "source": [
    "# Manually Creating a Document\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "nlp_large = spacy.load('en_core_web_lg')\n",
    "\n",
    "words = ['Hey', 'it', 'is', 'me', ',', 'Goku', '!']\n",
    "spaces = [True, True, True, False, True, False, False]\n",
    "document = Doc(nlp_large.vocab, words=words, spaces=spaces)\n",
    "\n",
    "print(f'- Document Text: {document}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a827cc78-eb1e-498a-9b8f-b0fc3166f00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Span Without Label: Goku - (label: )\n",
      "- Span With Label: Goku - (label: PERSON)\n"
     ]
    }
   ],
   "source": [
    "# Manually Creating a Span\n",
    "span_without_label = Span(document, 5, 6)\n",
    "span_with_label = Span(document, 5, 6, label='PERSON')\n",
    "\n",
    "print(f'- Span Without Label: {span_without_label} - (label: {span_without_label.label_})')\n",
    "print(f'- Span With Label: {span_with_label} - (label: {span_with_label.label_})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13a42754-e210-4265-9db4-cb8111e9dca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Document Named Entities (NER) - Before Manual Update:\n",
      "---\n",
      "- Document Named Entities (NER) - After Manual Update:\n",
      "Goku PERSON People, including fictional\n"
     ]
    }
   ],
   "source": [
    "# Updating Document Entities\n",
    "print('- Document Named Entities (NER) - Before Manual Update:')\n",
    "\n",
    "for entity in document.ents: print(entity.text, entity.label_, spacy.explain(entity.label_))\n",
    "\n",
    "###\n",
    "\n",
    "document.ents = [span_with_label]\n",
    "\n",
    "print('---\\n- Document Named Entities (NER) - After Manual Update:')\n",
    "\n",
    "for entity in document.ents: print(entity.text, entity.label_, spacy.explain(entity.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b394fb-9338-4501-8d20-69a4c348796a",
   "metadata": {},
   "source": [
    "<h1 id='2-word-vectors-and-semantic-similarity' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>ü™û | Word Vectors and Semantic Similarity</h1>\n",
    "\n",
    "`Semantic Similarity` is a technique to check out how similar Documents, Spans and Tokens are between each other given their content and not context. The similarity goes from 0 (`non-similar`) to 1 (`totally similar`) and it's calculated via `Cosine Similarity` by default in Spacy.\n",
    "\n",
    "In order to be able to calculate the similarity, the NLP object should have `WordVectors`, that are calculated via `Word2Vec` algorithm by default in Spacy.\n",
    "\n",
    "However, only medium (md) and large (lg) pre-trained Pipelines contain WordVectors automatically calculated when creating Documents, Spans and Tokens. So, when working with small (sm), accuracy (trf) or blank models, we should calculate the WordVectors by ourselves:\n",
    "\n",
    "- **‚ùå blank** - `doesn't contain WordVectors`;\n",
    "- **‚ùå en_core_web_sm** - `doesn't contain WordVectors`;\n",
    "- **‚ùå en_core_web_trf** - `doesn't contain WordVectors`;\n",
    "- **‚úîÔ∏è en_core_web_md** - `contains WordVectors`;\n",
    "- **‚úîÔ∏è en_core_web_lg** - `contains WordVectors`.\n",
    "\n",
    "Besides, short phrases are bettern than long Documents and Spans with many irrelevant words (Stop Words?!) when calculating Semantic Similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51ce2a5f-a517-48c4-9bac-b91272f6d5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- WordVectors Size: 300\n",
      "- WordVectors: [-1.81481451e-01  2.64698565e-01 -8.05035308e-02 -1.40889853e-01\n",
      "  9.04358849e-02 -3.57622840e-02  1.03283294e-01 -3.00455719e-01\n",
      " -1.44200295e-01  1.64712155e+00 -1.34329662e-01 -7.25031421e-02\n",
      " -1.61035825e-02  2.80598439e-02 -2.30228439e-01 -1.67791575e-01\n",
      "  2.05014311e-02  9.69782829e-01 -1.59310579e-01 -6.44387156e-02\n",
      "  7.81078562e-02 -6.00662865e-02  2.15001404e-02 -1.23481967e-01\n",
      " -1.98208541e-02  3.40822898e-02  3.25180031e-02 -9.37619954e-02\n",
      "  1.93001851e-01 -2.31685564e-01  3.92088555e-02  2.34101266e-01\n",
      "  5.11536486e-02  2.60485709e-02  2.27942858e-02 -2.28985283e-03\n",
      " -1.26268655e-01  5.40680028e-02 -1.43202186e-01 -1.15917541e-01\n",
      " -8.16669986e-02 -1.03729002e-01  2.71288585e-02 -3.72276567e-02\n",
      "  6.74497113e-02  2.21329853e-01 -2.21715704e-01 -1.78959861e-01\n",
      "  5.45985699e-02  8.39200336e-03  6.90828487e-02  2.34034270e-01\n",
      "  3.25114317e-02 -2.58411448e-02  1.49978492e-02  1.66501984e-01\n",
      "  1.89258587e-02  1.17471386e-02  7.05322847e-02 -8.19074288e-02\n",
      " -1.85557976e-01  5.15905805e-02 -1.13177575e-01 -2.12185774e-02\n",
      "  2.97984295e-02 -2.78999418e-01 -8.01434144e-02  1.19577713e-01\n",
      " -1.08082425e-02 -1.61094256e-02 -3.24932821e-02  1.07642710e-02\n",
      "  1.92269713e-01 -4.93204258e-02  3.22005711e-02  2.10891828e-01\n",
      "  2.20144719e-01 -1.48854151e-01  2.03757118e-02  5.24609983e-01\n",
      " -7.24457279e-02  1.56277418e-01 -7.92981386e-02  2.06977278e-01\n",
      "  1.10022858e-01 -2.83603400e-01  5.90884149e-01 -5.97292840e-01\n",
      "  1.98848844e-01 -3.12455725e-02  5.45715690e-02  1.26624718e-01\n",
      " -4.25254330e-02  8.34661573e-02  1.15996711e-01  3.08980495e-02\n",
      " -1.33763000e-01 -2.03363732e-01 -7.62488693e-02  1.28847569e-01\n",
      "  1.18421435e-01  5.67624308e-02 -2.49447435e-01  6.60834312e-02\n",
      "  6.99681491e-02 -2.06652835e-01  7.04245716e-02  8.50539953e-02\n",
      " -6.56671450e-02  3.29100271e-03  7.67004266e-02 -3.84542257e-01\n",
      " -1.14698578e-02 -2.21825719e-01  8.24703574e-02  1.99547157e-01\n",
      "  1.50731420e-02 -7.02042952e-02 -6.36497140e-02 -1.50498003e-01\n",
      " -1.29061555e-02 -2.67084222e-02  1.67554900e-01  1.09468140e-01\n",
      "  1.41245425e-01  1.61020592e-01 -8.72320905e-02 -2.47843713e-01\n",
      " -2.41172942e-03  7.39278570e-02 -1.27293989e-01 -2.12534145e-01\n",
      " -1.97685626e-03 -2.34812889e-02  4.60369997e-02 -5.78931496e-02\n",
      " -5.49054332e-02 -1.13971401e-02 -1.65472001e-01  7.88690001e-02\n",
      " -1.87804008e+00  2.32644290e-01  2.14362144e-01  5.53934276e-02\n",
      "  2.74610519e-03  9.96640101e-02  2.09385697e-02  8.28286540e-03\n",
      " -2.20678002e-01 -5.25637157e-02 -8.93344060e-02  1.93064138e-01\n",
      "  2.76194006e-01 -2.61248592e-02  1.33435488e-01 -5.52561469e-02\n",
      " -1.59794260e-02 -2.82907158e-01 -6.35428960e-03  2.05285428e-03\n",
      " -1.49486109e-03  9.06248577e-03 -3.33274268e-02 -6.30697161e-02\n",
      " -8.87652040e-02 -1.08240880e-01  2.80635748e-02  3.80862579e-02\n",
      " -4.53639887e-02  9.01398584e-02 -1.57816127e-01 -7.72787407e-02\n",
      " -6.56752810e-02 -2.04027146e-01 -2.37760320e-01 -4.70711328e-02\n",
      " -1.32351562e-01 -1.99702885e-02 -2.07296923e-01 -6.21708557e-02\n",
      "  3.22059989e-02 -7.20648617e-02 -4.27688621e-02 -1.67978317e-01\n",
      "  8.76422971e-02 -1.96677849e-01 -1.31236285e-01 -7.96891451e-02\n",
      "  5.76658584e-02  9.61325690e-02  1.15915872e-01 -1.09257251e-02\n",
      " -3.19148570e-01  3.68785672e-02  4.91005704e-02 -2.07800008e-02\n",
      " -5.95211005e-03 -2.28088722e-01  2.12014571e-01  2.31619999e-01\n",
      " -5.83999977e-02  4.32535596e-02 -5.28207198e-02  2.90265698e-02\n",
      "  2.29148433e-01 -7.74185732e-02  8.36975724e-02 -5.10377102e-02\n",
      "  1.90184712e-01 -1.19220294e-01 -1.74570173e-01 -2.48286128e-01\n",
      " -4.10054699e-02  4.90807220e-02  1.83762029e-01  1.03966549e-01\n",
      "  1.29913285e-01  1.61900595e-02 -5.74031472e-02  4.32778597e-02\n",
      " -3.46469991e-02  1.02558427e-01 -3.85422818e-02  5.44617474e-02\n",
      "  1.23729996e-01  5.50610013e-02 -8.53038654e-02  2.23073006e-01\n",
      "  8.67472291e-02 -1.22613847e-01 -2.48125300e-01 -1.14730000e-02\n",
      "  1.18141407e-02  1.21587574e-01 -1.33197576e-01  2.25733146e-01\n",
      "  6.51213760e-03 -1.43849880e-01 -1.27420574e-01  4.59771417e-02\n",
      "  5.23349978e-02  5.76881468e-02 -3.43997143e-02  1.76564261e-01\n",
      "  1.56947270e-01 -2.01298147e-01 -6.09922782e-03 -6.60212860e-02\n",
      "  8.81072879e-02  3.15473735e-01  1.34921998e-01 -1.89203516e-01\n",
      " -8.86107162e-02  6.69428287e-03  4.08527181e-02  1.57522708e-01\n",
      "  5.02547100e-02 -9.26415771e-02  5.72413858e-03  1.49715738e-02\n",
      "  1.20523706e-01  8.65358636e-02  1.67622864e-02  6.84184283e-02\n",
      " -2.21308637e-02 -1.04266442e-01  7.46556697e-03  1.33078709e-01\n",
      "  1.65187433e-01  1.89571723e-01 -9.34711541e-04 -1.26134425e-01\n",
      " -1.71600103e-01 -2.49172896e-02 -1.93709001e-01  4.31635641e-02\n",
      " -1.34843007e-01 -1.67292267e-01  1.28058895e-01  2.75621265e-01\n",
      "  1.09842859e-01 -5.83967157e-02  3.06274705e-02 -1.24902971e-01\n",
      "  2.33478565e-02 -4.69555780e-02  2.36890152e-01 -1.52906865e-01\n",
      "  1.17684424e-01 -1.99263111e-01  4.10732813e-02 -6.78437203e-02\n",
      " -4.72642854e-02 -4.09389995e-02  2.33898729e-01 -7.78134316e-02\n",
      " -5.71514294e-02 -7.92845711e-02 -3.87699008e-02  1.68674484e-01]\n"
     ]
    }
   ],
   "source": [
    "# Accessing WordVector object of a Document via '.vector' list\n",
    "document0 = nlp_large('Hey it is me, Goku!')\n",
    "print(f'- WordVectors Size: {len(document0.vector)}')\n",
    "print(f'- WordVectors: {document0.vector}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88f15434-86c1-447d-88ef-a24b8695f6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- WordVectors Size: 300\n",
      "- WordVectors: [-4.0001e-01 -6.4200e-01  5.4013e-01 -4.6932e-01  2.3678e-01 -2.1087e-01\n",
      "  6.1721e-01  1.7844e-01 -3.0992e-02 -8.0075e-01 -2.2622e-02 -5.7395e-01\n",
      " -2.0335e-01  5.6272e-01 -2.1141e-01 -1.8668e-01  4.7549e-01  2.4373e-01\n",
      " -3.3346e-01 -2.6711e-01  4.7203e-01 -2.5117e-01  2.1239e-01 -9.1873e-01\n",
      " -1.7530e-01  3.1297e-01 -3.4612e-02 -3.0685e-01 -6.7977e-02 -3.3142e-01\n",
      "  8.2209e-02  6.5252e-02  5.3315e-01 -4.8743e-02  1.8212e-01 -2.1009e-01\n",
      " -9.8826e-01 -1.4896e-01 -5.1341e-01 -2.7604e-02 -4.3536e-01 -9.4728e-01\n",
      "  2.3615e-01  4.6428e-01 -2.3510e-01  1.9391e-01 -1.4218e-01 -4.6251e-01\n",
      " -1.9442e-01 -4.2031e-01  5.2424e-02 -3.2733e-01  5.8309e-01 -5.6361e-01\n",
      " -2.2144e-01  1.0501e+00 -1.4222e-02  3.3922e-01  1.4471e-01 -4.2506e-01\n",
      "  3.8405e-02  5.8688e-01 -1.6905e-01 -4.0435e-01 -2.8506e-01 -7.6365e-02\n",
      "  1.0794e-01  4.7223e-01  4.4349e-02 -4.1628e-01 -4.7326e-01 -4.2144e-02\n",
      "  1.1894e-01  1.2493e-02  3.9658e-02  7.8624e-01  4.5608e-01 -2.4868e-01\n",
      "  6.6457e-01  6.0021e-01 -2.2850e-01  5.3138e-01  7.4252e-02  3.8885e-01\n",
      "  1.2449e-01  1.2114e-02  1.2864e+00 -7.9519e-01 -7.1542e-01 -5.6401e-01\n",
      " -1.7614e-01  2.9771e-01 -5.7507e-01 -5.2980e-02  2.3526e-01 -1.4328e-02\n",
      "  3.0203e-02 -4.5622e-01  5.4788e-01  2.1158e-01  6.6112e-01  6.7305e-01\n",
      " -3.4382e-01  2.8099e-01  1.3968e-01 -9.8726e-01 -1.7077e-01  3.6486e-01\n",
      "  1.7935e-01  2.3934e-02  1.4860e-01 -8.0594e-01 -1.1645e+00 -9.9446e-01\n",
      "  6.4812e-01  1.0880e+00  1.4618e-01  5.6402e-01 -1.4049e-01 -8.6346e-02\n",
      " -7.1477e-01  3.7064e-01  3.8566e-01  6.3988e-01  6.0357e-01  5.5763e-01\n",
      " -7.3151e-01 -4.6396e-01 -2.4343e-01  4.1922e-01 -8.6735e-01  6.9078e-02\n",
      "  2.0558e-01 -3.5848e-01 -6.2264e-02 -5.3331e-02  1.8893e-01  3.3667e-02\n",
      " -4.2068e-01  4.0396e-01 -1.9623e+00 -4.9793e-01 -4.0744e-01  1.2224e-01\n",
      "  8.2833e-03  7.8070e-01  3.4247e-01 -5.8833e-01 -5.6036e-01 -1.9274e-01\n",
      " -1.2193e-01  1.7476e-01  7.9979e-01 -1.2130e-01  2.4060e-01 -2.7522e-01\n",
      " -3.1650e-01 -3.2148e-01  3.3525e-01  4.8111e-01  2.5161e-01 -1.5320e-01\n",
      " -2.5478e-01  3.7141e-02  2.6646e-01 -6.5442e-01 -1.7368e-01  3.8886e-01\n",
      " -6.1952e-01 -3.9240e-02 -7.0462e-01 -1.4632e-03 -1.9323e-01 -1.5834e-01\n",
      " -6.8510e-01 -5.0341e-01 -4.8850e-01  5.7003e-01 -3.5946e-01  6.2104e-02\n",
      "  2.0115e-01  2.1873e-01  4.1476e-01 -3.8281e-01  1.9678e-01 -3.9696e-02\n",
      " -1.0645e-01 -2.6035e-01  1.1005e-01  2.5890e-01 -1.8186e-02  6.0195e-01\n",
      " -1.1712e+00  3.6529e-01  2.4761e-01 -5.7059e-01 -1.9877e-04 -5.0384e-02\n",
      "  1.5967e-01  1.5423e-01  6.9353e-01 -1.2648e-01  6.7681e-01  5.5164e-01\n",
      "  6.7908e-01 -1.2545e-01 -6.4718e-02 -5.9634e-01  2.2663e-01 -1.7617e-01\n",
      "  1.2049e-03  1.5678e-02 -2.0383e-01  8.9327e-01  1.6602e-01 -6.0776e-01\n",
      "  5.2095e-01  3.6794e-01  6.7023e-01 -1.5680e-01  2.7839e-01 -5.0026e-01\n",
      "  9.1281e-02 -2.8618e-03 -2.4985e-01  1.1022e-01 -2.3338e-01  6.2311e-02\n",
      "  7.5049e-01 -1.3958e-02 -5.5921e-01 -6.1496e-01 -6.7822e-02  1.0072e-01\n",
      " -6.1184e-01  1.3712e+00  3.7505e-02 -2.0750e-01 -1.6233e-01 -5.4304e-01\n",
      " -2.7253e-01  2.1501e-02 -3.8133e-01  5.0279e-01  1.3086e-01 -4.5309e-01\n",
      " -5.2407e-02  3.5977e-01  5.2243e-01  7.1908e-02  1.2441e-01 -8.7933e-01\n",
      " -5.4115e-02  2.1343e-01  5.2629e-01  9.7995e-02 -3.5835e-01  4.2639e-02\n",
      " -2.6502e-02 -2.0983e-02  1.6814e-01  1.7620e-01  1.7536e-01  1.9299e-01\n",
      "  5.8563e-02 -1.5994e-01 -6.8927e-01  6.7921e-01 -8.4516e-01  4.9028e-01\n",
      "  2.8299e-01 -3.4533e-01  1.4554e-01  2.5961e-01 -1.5540e-01  1.4720e-01\n",
      " -9.9668e-01 -4.3692e-01  4.0953e-01  3.3818e-01 -6.1039e-01  4.6887e-02\n",
      "  8.8048e-03 -8.7238e-03  4.3988e-01  1.6086e-01  3.7858e-01 -5.3681e-01\n",
      " -1.3553e-01 -7.3839e-01  9.6171e-01  1.4430e-01 -1.7855e-01 -1.6132e-01\n",
      "  3.3277e-01  4.3376e-02 -3.0405e-01 -1.2109e-01  5.4114e-01  7.4098e-01]\n"
     ]
    }
   ],
   "source": [
    "# Accessing WordVector object of a Token via '.vector' list\n",
    "token0 = document0[5]\n",
    "print(f'- WordVectors Size: {len(token0.vector)}')\n",
    "print(f'- WordVectors: {token0.vector}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8dcd2d-3e9f-4855-9443-ea978d8b5571",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff16d5dc-d385-4251-b5ed-c1787125345d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Similarity between Documents: 0.9358318464113806\n"
     ]
    }
   ],
   "source": [
    "# Similarity between Documents\n",
    "document1 = nlp_large('I love pizza')\n",
    "document2 = nlp_large('I love pasta')\n",
    "print(f'- Similarity between Documents: {document1.similarity(document2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1166409-81e1-4fca-8f00-041c24c0fc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Similarity between Tokens: 0.7369545698165894\n"
     ]
    }
   ],
   "source": [
    "# Similarity between Tokens\n",
    "document3 = nlp_large('I love pizza and pasta')\n",
    "token1 = document3[2]\n",
    "token2 = document3[4]\n",
    "print(f'- Similarity between Tokens: {token1.similarity(token2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9527e77-ea66-40ed-86f6-575785d37034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Similarity between Document and Token: 0.5415431108130979\n"
     ]
    }
   ],
   "source": [
    "# Similarity between Document and Token\n",
    "document4 = nlp_large('I love pizza')\n",
    "token = nlp_large('cheese')[0]\n",
    "print(f'- Similarity between Document and Token: {document4.similarity(token)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e58c3375-cad5-46b0-9ec1-f7602a1cf035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Similarity between Span and Document: 0.5886225771237401\n",
      "- Similarity between Document and Span: 0.5886225771237401\n"
     ]
    }
   ],
   "source": [
    "# Similarity between Span and Document\n",
    "document5 = nlp_large('McDonalds sells burger')\n",
    "span = nlp_large('I like pizza and pasta')[2:5]\n",
    "print(f'- Similarity between Span and Document: {span.similarity(document5)}')\n",
    "print(f'- Similarity between Document and Span: {document5.similarity(span)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56f6d9c-7352-4ad3-ad67-4773689aae8f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Besides, similarity doesn't recognize `sentiments`. For instance, the two phrases `I love cats` and `I hate cats` have high similarity even though their meanings are totally opposite.\n",
    "\n",
    "It happens due to their semantic contents be very similar: both contains the words `I` followed by a `VERB` and then the word `cats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2de6e02b-79af-4b0c-a32c-afb1e683df4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Similarity between Documents: 0.9409261755229907\n"
     ]
    }
   ],
   "source": [
    "# Semantic Similarity doesn't consider Sentiments but only Contents\n",
    "document6 = nlp_large('I love cats')\n",
    "document7 = nlp_large('I hate cats')\n",
    "print(f'- Similarity between Documents: {document6.similarity(document7)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e10e729-4525-4efd-95ee-eca5c8f3b2e7",
   "metadata": {},
   "source": [
    "<h1 id='3-combining-predictions-and-rules' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üé® | Combining Predictions and Rules</h1>\n",
    "\n",
    "The combination of Predictions from `Statistical Models` and Rules from `Rule-Based Systems` is a powerful technique to boost searches and Document processings.\n",
    "\n",
    "They are, literally, the combination of `context-dependent` and `context-independent` (both predictions) and texts (rules) during searches and matches over the Documents. So:\n",
    "\n",
    "- **Statistical Models** - `searches for generalized info, such as Named Entities (NER), Part-of-Speech (POS), Dependency Label and Syntatic Head of Tokens`;\n",
    "- **Rule-Based Systems** - `searches for specific, finite info, such as Specific Named Entities (countries of the world, soccer player names and dog breeds). We can achieve it by using Tokenizer, Matcher and PhraseMatcher objects from Spacy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "196445d7-497d-44af-bd22-64fc0e9be311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Text: Golden Retriever\n",
      "- Root: Retriever\n",
      "- Part-of-Speech (POS): PROPN (proper noun)\n",
      "- Dependency Label: dobj (direct object)\n",
      "- Syntatic Head: have\n",
      "- Previous Token: a\n"
     ]
    }
   ],
   "source": [
    "# Combining Predictions from Statistical Modes\n",
    "# and Rules from Rule-Based Systems\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "document = nlp_large('I have a Golden Retriever')\n",
    "\n",
    "pattern = [{ 'LOWER': 'golden' }, { 'LOWER': 'retriever' }]\n",
    "\n",
    "matcher = Matcher(nlp_large.vocab)\n",
    "matcher.add('DOG_BREED', [pattern])\n",
    "matches = matcher(document)\n",
    "\n",
    "for match_id, start_index, end_index in matches:\n",
    "    matched_span = document[start_index:end_index]\n",
    "    print(f'- Text: {matched_span.text}')\n",
    "    print(f'- Root: {matched_span.root.text}') # Token that decides the Category of the Span\n",
    "    print(f'- Part-of-Speech (POS): {matched_span.root.pos_} ({spacy.explain(matched_span.root.pos_)})')\n",
    "    print(f'- Dependency Label: {matched_span.root.dep_} ({spacy.explain(matched_span.root.dep_)})')\n",
    "    print(f'- Syntatic Head: {matched_span.root.head.text}')\n",
    "    print(f'- Previous Token: {document[start_index-1].text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00992a0f-5976-48c4-aad0-085f8ee346c2",
   "metadata": {},
   "source": [
    "<h1 id='4-phrasematcher' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üîç | PhraseMatcher</h1>\n",
    "\n",
    "Differently to Matcher, `PhraseMatchers` are more efficient and faster when we desire to search for a specific list of strings and instead of receiving a dictionaries with search rules, it receives only the list of strings that we desire to search and match.\n",
    "\n",
    "Oh, and always remember to convert this list into a `nlp.pipe` in order to gain more efficiency and save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85494afb-5dc0-4129-b068-7cdba2ce7c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Matched Span: Golden Retriever\n"
     ]
    }
   ],
   "source": [
    "# PhraseMatcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "document = nlp_large('I have a Golden Retriever')\n",
    "\n",
    "pattern = ['Golden Retriever']\n",
    "\n",
    "phraseMatcher = PhraseMatcher(nlp_large.vocab)\n",
    "phraseMatcher.add('DOG_BREED', nlp_large.pipe(pattern))\n",
    "matches = phraseMatcher(document)\n",
    "\n",
    "for match_id, start_index, end_index in matches:\n",
    "    matched_span = document[start_index:end_index]\n",
    "    print(f'- Matched Span: {matched_span.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caca4399-efd5-49f9-9725-44987323f6b9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h1 id='reach-me' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>üì´ | Reach Me</h1>\n",
    "\n",
    "> **Email** - [csfelix08@gmail.com](mailto:csfelix08@gmail.com?)\n",
    "\n",
    "> **Linkedin** - [linkedin.com/in/csfelix/](https://www.linkedin.com/in/csfelix/)\n",
    "\n",
    "> **GitHub:** - [CSFelix](https://github.com/CSFelix)\n",
    "\n",
    "> **Kaggle** - [DSFelix](https://www.kaggle.com/dsfelix)\n",
    "\n",
    "> **Portfolio** - [CSFelix.io](https://csfelix.github.io/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
